{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    classification_report, \n",
    "    confusion_matrix, \n",
    "    ConfusionMatrixDisplay, \n",
    "    f1_score,\n",
    "    cohen_kappa_score\n",
    ")\n",
    "from sklearn.utils import resample\n",
    "#from workflow import decision_logic\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# transformers==4.50.0\n",
    "# attention implementation = flash_attention_2\n",
    "\n",
    "# microsoft/Phi-3.5-mini-instruct\n",
    "# -------------------------------\n",
    "#timestamp = \"2025-04-29_19:09\" # 1 example \n",
    "#timestamp = \"2025-04-29_20:32\" # 5 examples\n",
    "#timestamp = \"2025-05-05_12:30\" # 10 examples\n",
    "\n",
    "# microsoft/Phi-3-small-8k-instruct\n",
    "# ---------------------------------\n",
    "#timestamp = \"2025-05-14_13:17\" # 1 example\n",
    "#timestamp = \"2025-05-14_13:22\" # 5 examples\n",
    "#timestamp = \"2025-05-14_13:24\" # 10 examples\n",
    "\n",
    "# allenai/Llama-3.1-Tulu-3.1-8B\n",
    "# ------------------------------\n",
    "#timestamp = \"2025-05-14_15:30\" # 1 example\n",
    "#timestamp = \"2025-05-15_18:42\" # 5 examples\n",
    "#timestamp = \"2025-05-14_16:38\" # 10 examples\n",
    "\n",
    "# CohereForAI/c4ai-command-r7b-12-2024\n",
    "# ------------------------------------\n",
    "#timestamp = \"2025-05-14_17:30\" # 1 example\n",
    "#timestamp = \"2025-05-14_16:30\" # 5 examples # average F1 across themes .81\n",
    "#timestamp = \"2025-05-14_19:27\" # 10 examples\n",
    "\n",
    "# google/gemma-2-9b-it\n",
    "# --------------------\n",
    "#timestamp = \"2025-05-14_21:24\" # 1 example\n",
    "#timestamp = \"2025-05-14_20:30\" # 5 examples\n",
    "#timestamp = \"2025-05-15_00:07\" # 10 examples\n",
    "\n",
    "# CohereForAI for theme detection, and google gemma for target classification, with 5 examples\n",
    "# DROP SPORTS!\n",
    "\n",
    "timestamp = \"2025-06-19_15:00\" # 5 examples\n",
    "\n",
    "\n",
    "verbose = True\n",
    "bootstrap_flag = True\n",
    "\n",
    "# load config, predictions and groundtruth\n",
    "with open(os.path.join(\"results\", timestamp, \"config.json\"), \"r\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "cv_folds = os.path.join(\"results\", timestamp, \"cv_folds.json\")\n",
    "if os.path.exists(cv_folds):\n",
    "    with open(cv_folds, \"r\") as f:\n",
    "        cv_folds = json.load(f)\n",
    "else:\n",
    "    cv_folds = None\n",
    "\n",
    "if verbose:\n",
    "    # print main parameters\n",
    "    print(f\"Main parameters for experiment {timestamp}:\")\n",
    "    for key, value in config.items():\n",
    "        print(f\"   - {key}: {value}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "groundtruth_df = pd.read_csv(config[\"groundtruth\"], index_col=\"stimulus_id\")\n",
    "\n",
    "def bootstrap(y_true, y_pred, frac=0.25, iterations=10000, alpha=0.95):\n",
    "    f1s = []\n",
    "    cohen_kappas = []\n",
    "    for _ in range(iterations):\n",
    "        # resample the data\n",
    "        y_true_resampled, y_pred_resampled = resample(y_true, y_pred, n_samples=int(frac * len(y_true)), random_state=None)\n",
    "        f1s.append(f1_score(y_true_resampled, y_pred_resampled, average='macro'))\n",
    "        cohen_kappas.append(cohen_kappa_score(y_true_resampled, y_pred_resampled))\n",
    "\n",
    "    # F1 confidence intervals\n",
    "    p = ((1.0-alpha)/2.0) * 100\n",
    "    lower = max(0.0, np.percentile(f1s, p))\n",
    "    p = (alpha+((1.0-alpha)/2.0)) * 100\n",
    "    upper = min(1.0, np.percentile(f1s, p))\n",
    "    print(f\" --- {alpha*100}% F1 confidence interval = [{lower:.2f}, {upper:.2f}]\")\n",
    "\n",
    "    # Cohen's kappa confidence intervals\n",
    "    p = ((1.0-alpha)/2.0) * 100\n",
    "    lower_kappa = max(0.0, np.percentile(cohen_kappas, p))\n",
    "    p = (alpha+((1.0-alpha)/2.0)) * 100\n",
    "    upper_kappa = min(1.0, np.percentile(cohen_kappas, p))\n",
    "    print(f\" --- {alpha*100}% Cohen's kappa confidence interval = [{lower_kappa:.2f}, {upper_kappa:.2f}]\")\n",
    "\n",
    "\n",
    "\n",
    "def final_cleanup(result):\n",
    "    if \"boys\" in result.lower():\n",
    "        return \"Boys/men\"\n",
    "    if \"girls\" in result.lower():\n",
    "        return \"Girls/women\"\n",
    "    if \"mixed\" in result.lower():\n",
    "        return \"Mixed\"\n",
    "    else:\n",
    "        return \"placeholder\"\n",
    "\n",
    "def decision_logic(tasks_results):\n",
    "    results = [tasks_results[task][\"result\"] for task in tasks_results.keys()]\n",
    "\n",
    "    results = [x for x in results if isinstance(x, str)]\n",
    "    results = [x for x in results if x != \"placeholder\"]\n",
    "\n",
    "    if not results:\n",
    "        return \"placeholder\"\n",
    "\n",
    "    counts = {x: results.count(x) for x in set(results)}\n",
    "    \n",
    "    max_count = max(counts.values())\n",
    "    winners = [k for k, v in counts.items() if v == max_count]\n",
    "\n",
    "    if len(winners) == 1:\n",
    "        result = winners[0]\n",
    "        #return winners[0]\n",
    "    else: # len(winners) > 1:\n",
    "        # if there's a tie, return the result of the task containing the winners\n",
    "        tie_task = [\n",
    "            task for task in tasks_results.keys() \n",
    "            if set(tasks_results[task][\"classes\"]) == set(winners)\n",
    "        ]\n",
    "        if len(tie_task) == 1:\n",
    "            result = tasks_results[tie_task[0]][\"result\"]\n",
    "        else:\n",
    "            # this should never happen, but just in case\n",
    "            result = max(set(results), key=results.count)\n",
    "    \n",
    "    return final_cleanup(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_files = glob.glob(f\"results/{timestamp}/*_target_classification.json\")\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "y_pred_without_music_in_subtasks = []\n",
    "y_pred_gmb = []\n",
    "\n",
    "for file in prediction_files:\n",
    "    stimulus_id = os.path.split(file)[1].split(\"_target_classification\")[0]\n",
    "\n",
    "    y_true.append(groundtruth_df.loc[stimulus_id, \"target_of_toy_ad\"])\n",
    "\n",
    "    #with open(file, \"r\") as f:\n",
    "    #    data = json.load(f)\n",
    "    #    y_pred.append(data[\"target_class\"])\n",
    "\n",
    "    with open(f\"results/{timestamp}/{stimulus_id}_tasks_results.json\", \"r\") as f:\n",
    "        tasks_results = json.load(f)\n",
    "\n",
    "    y_pred.append(decision_logic(tasks_results))\n",
    "\n",
    "    tasks_results.pop(\"music\", None)\n",
    "    y_pred_without_music_in_subtasks.append(decision_logic(tasks_results))\n",
    "    y_pred_gmb.append(final_cleanup(tasks_results[\"G/M/B\"][\"result\"]))\n",
    "\n",
    "# remove parsing errors in main logic\n",
    "parsing_errors_idx = [i for i, x in enumerate(y_pred) if x not in groundtruth_df.target_of_toy_ad.unique()] \n",
    "y_pred = [x for i, x in enumerate(y_pred) if i not in parsing_errors_idx]\n",
    "y_true_without_parsing_errors = [x for i, x in enumerate(y_true) if i not in parsing_errors_idx]\n",
    "\n",
    "print(f\"There were {len(parsing_errors_idx)} parsing errors during target classification (main logic).\")\n",
    "for i in parsing_errors_idx:\n",
    "    filemsg = f\"   - {prediction_files[i].split('/')[-1]}\"\n",
    "    stimulus_id = os.path.split(prediction_files[i])[1].split(\"_target_classification\")[0]\n",
    "    # find which fold the stimulus belongs to\n",
    "    if cv_folds is not None:\n",
    "        for fold in range(len(cv_folds[\"test_original_ids\"])):\n",
    "            if stimulus_id in cv_folds[\"test_original_ids\"][fold]:\n",
    "                filemsg += f\" in fold {fold} (0-ind)\"\n",
    "                break\n",
    "        else:\n",
    "            filemsg += f\" in fold (under computation)\"\n",
    "    print(filemsg)\n",
    "print(\"\\n\")\n",
    "\n",
    "# remove parsing errors in secondary logics\n",
    "parsing_errors_idx = [i for i, x in enumerate(y_pred_without_music_in_subtasks) if x not in groundtruth_df.target_of_toy_ad.unique()]\n",
    "if parsing_errors_idx:\n",
    "    print(f\"There were {len(parsing_errors_idx)} parsing errors during target classification (without music in subtasks).\")\n",
    "\n",
    "y_pred_without_music_in_subtasks = [x for i, x in enumerate(y_pred_without_music_in_subtasks) if i not in parsing_errors_idx]\n",
    "y_true_without_music_in_subtasks = [x for i, x in enumerate(y_true) if i not in parsing_errors_idx]\n",
    "\n",
    "\n",
    "parsing_errors_idx = [i for i, x in enumerate(y_pred_gmb) if x not in groundtruth_df.target_of_toy_ad.unique()]\n",
    "if parsing_errors_idx:\n",
    "    print(f\"There were {len(parsing_errors_idx)} parsing errors during target classification (G/M/B).\")\n",
    "\n",
    "y_pred_gmb = [x for i, x in enumerate(y_pred_gmb) if i not in parsing_errors_idx]\n",
    "y_true_gmb = [x for i, x in enumerate(y_true) if i not in parsing_errors_idx]\n",
    "\n",
    "\n",
    "print(\"Classification report (main logic):\")\n",
    "print(classification_report(y_true_without_parsing_errors, y_pred))\n",
    "\n",
    "\n",
    "print(f\"F1 (main logic): {f1_score(y_true_without_parsing_errors, y_pred, average='macro'):.2f}\")\n",
    "print(f\"Cohen's kappa (main logic): {cohen_kappa_score(y_true_without_parsing_errors, y_pred):.2f}\")\n",
    "if bootstrap_flag:\n",
    "    bootstrap(y_true_without_parsing_errors, y_pred, frac=0.25, iterations=10000, alpha=0.95)\n",
    "print(f\"\\nF1 (without music in subtasks): {f1_score(y_true_without_music_in_subtasks, y_pred_without_music_in_subtasks, average='macro'):.2f}\")\n",
    "print(f\"Cohen's kappa (without music in subtasks): {cohen_kappa_score(y_true_without_music_in_subtasks, y_pred_without_music_in_subtasks):.2f}\")\n",
    "if bootstrap_flag:\n",
    "    bootstrap(y_true_without_music_in_subtasks, y_pred_without_music_in_subtasks, frac=0.25, iterations=10000, alpha=0.95)\n",
    "print(f\"\\nF1 (G/M/B): {f1_score(y_true_gmb, y_pred_gmb, average='macro'):.2f}\")\n",
    "print(f\"Cohen's kappa (G/M/B): {cohen_kappa_score(y_true_gmb, y_pred_gmb):.2f}\")\n",
    "if bootstrap_flag:\n",
    "    bootstrap(y_true_gmb, y_pred_gmb, frac=0.25, iterations=10000, alpha=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_folds.keys()\n",
    "music_only_f1s = []\n",
    "for fold in range(len(cv_folds[\"test_original_ids\"])):\n",
    "    music_only_f1s.append(\n",
    "        f1_score(\n",
    "            cv_folds[\"actual\"][fold],\n",
    "            cv_folds[\"music_only_predictions\"][fold],\n",
    "            average=\"macro\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "print(\"NB: this is a 10-fold cross-validation\")\n",
    "print(f\"F1 (music only): {np.mean(music_only_f1s):.2f} (std: {np.std(music_only_f1s):.2f})\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Having \"Mixed\" in the middle improves readability of the confusion matrix\n",
    "target_names = [\"Girls/women\", \"Mixed\", \"Boys/men\"] \n",
    "\n",
    "# labels to indices\n",
    "y_true_without_parsing_errors = [target_names.index(x) for x in y_true_without_parsing_errors]\n",
    "y_pred = [target_names.index(x) for x in y_pred]\n",
    "\n",
    "cm = confusion_matrix(y_true_without_parsing_errors, y_pred)\n",
    "\n",
    "# normalize column-wise (precision estimate in the diagonal)\n",
    "cm_col = cm.astype('float') / cm.sum(axis=0)[np.newaxis, :]\n",
    "cm_col = np.round(cm_col, 2)\n",
    "\n",
    "# normalize row-wise (recall in the diagonal)\n",
    "cm_row = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "cm_row = np.round(cm_row, 2)\n",
    "\n",
    "# plot the confusion matrix\n",
    "for cm, title in zip([cm_col, cm_row], [\"Precision in the diagonal\", \"Recall in the diagonal\"]):\n",
    "    # Plot confusion matrix with labels\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=target_names)\n",
    "    disp.plot()\n",
    "    # add title\n",
    "    plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import permutations\n",
    "from pprint import pprint\n",
    "\n",
    "print_missclassified = False\n",
    "\n",
    "if print_missclassified:\n",
    "    for y_true_label, y_pred_label in list(permutations([0, 1, 2], 2)): # all non-diagonal cells\n",
    "        # get the indices of the misclassified samples\n",
    "        idxs = [i for i, (a, b) in enumerate(zip(y_true, y_pred)) if a == y_true_label and b == y_pred_label]\n",
    "        print(f\"\\033[1m\\033[31m| True: {target_names[y_true_label]} | Pred: {target_names[y_pred_label]} |\\033[0m\")\n",
    "        for idx in idxs:\n",
    "            with open(prediction_files[idx], \"r\") as f:\n",
    "                state = json.load(f)\n",
    "\n",
    "            stimulus_id = os.path.split(prediction_files[idx])[1].split(\"_target_classification\")[0]\n",
    "            fp = f\"results/{timestamp}/{stimulus_id}_tasks_results.json\"\n",
    "            with open(fp, \"r\") as f:\n",
    "                tasks_results = json.load(f)\n",
    "\n",
    "            tasks_results = {key: tasks_results[key][\"result\"] for key in tasks_results}\n",
    "\n",
    "            collected_themes = []\n",
    "            for theme in state[\"collected_cues\"].keys():\n",
    "                if len(state[\"collected_cues\"][theme][\"cues\"]) > 0:\n",
    "                    collected_themes.append({\n",
    "                        theme+\"_cues\": state[\"collected_cues\"][theme][\"cues\"]\n",
    "                        }\n",
    "                    )\n",
    "            \n",
    "            print(f\"- {prediction_files[idx].split('/')[-1]}\")\n",
    "            disp = {\n",
    "                \"----------\": {\n",
    "                    \"transcript\": state[\"current_transcript\"],\n",
    "                    \"collected_themes\": collected_themes,\n",
    "                    \"tasks_results\": tasks_results,\n",
    "                },\n",
    "            }\n",
    "\n",
    "            pprint(disp, indent=1, width=100, depth=None, sort_dicts=True)\n",
    "\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "themes_preds = {\n",
    "    theme : {\n",
    "        \"y_true\":[],\n",
    "        \"y_pred\":[],\n",
    "    } \n",
    "    for theme in config[\"themes_definitions\"].keys()\n",
    "}\n",
    "for file in prediction_files:\n",
    "    with open(file, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    stimulus_id = os.path.split(file)[1].split(\"_target_classification\")[0]\n",
    "\n",
    "    for theme in config[\"themes_definitions\"].keys():\n",
    "        collected_cues = data[\"collected_cues\"][theme]\n",
    "        true_cues = groundtruth_df.loc[stimulus_id, theme+\"_cues\"]\n",
    "        theme_true = 0 if pd.isna(true_cues) else 1\n",
    "        theme_pred = 1 if collected_cues[\"cues\"] else 0\n",
    "\n",
    "        themes_preds[theme][\"y_true\"].append(theme_true)\n",
    "        themes_preds[theme][\"y_pred\"].append(theme_pred)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1s = []\n",
    "cohen_kappas = []\n",
    "for theme in themes_preds.keys():\n",
    "    print(f\"Theme {theme}:\")\n",
    "    f1 = f1_score(themes_preds[theme][\"y_true\"], themes_preds[theme][\"y_pred\"], average=\"macro\")\n",
    "    f1s.append(f1)\n",
    "    print(f\"F1: {f1:.2f}\")\n",
    "\n",
    "    cohen_kappa = cohen_kappa_score(themes_preds[theme][\"y_true\"], themes_preds[theme][\"y_pred\"])\n",
    "    cohen_kappas.append(cohen_kappa)\n",
    "    print(f\"Cohen's kappa: {cohen_kappa:.2f}\")\n",
    "\n",
    "    if bootstrap_flag:\n",
    "        bootstrap(themes_preds[theme][\"y_true\"], themes_preds[theme][\"y_pred\"], frac=0.25, iterations=10000, alpha=0.95)\n",
    "\n",
    "    print(\"\\n\")\n",
    "\n",
    "print(f\"Average F1 across themes: {np.mean(f1s):.2f} ± {np.std(f1s):.2f}\")\n",
    "print(f\"Average Cohen's kappa across themes: {np.mean(cohen_kappas):.2f} ± {np.std(cohen_kappas):.2f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataset_tools_and_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
